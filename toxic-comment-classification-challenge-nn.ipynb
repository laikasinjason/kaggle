{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_auc_score\n\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,Add\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nimport lightgbm as lgb\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom scipy import sparse\nimport re, string\n%matplotlib inline\nseed = 42\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef pr(x, y_i, y):\n    y = y.values\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)\n\ndef clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    return comment\n\ndef enrich_indirect_features(df):\n    #Word count in each comment:\n    df['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n    #Unique word count\n    df['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n    #punctuation count\n    df[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    #derived features\n    #Word count percent in each comment:\n    df['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n    df['word_unique_percent'].fillna(1,inplace=True)\n    #derived features\n    #Punct percent in each comment:\n    df['punct_percent']=df['count_punctuations']*100/df['count_word']\n    df['punct_percent'].fillna(0,inplace=True)\n    df.drop(['count_word', 'count_unique_word'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"start_time = time.time()\npath = '../input/jigsaw-toxic-comment-classification-challenge/'\ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')\nprint('[{}] Finished to load data'.format(time.time() - start_time))\nprint('Number of rows and columns in the train data set:',train.shape)\nprint('Number of rows and columns in the test data set:',test.shape)\n\n# create submission file\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\n\nnrow_train = train.shape[0]\nmerge: pd.DataFrame = pd.concat([train, test])\n\n# train dun have Nan values\nmerge.fillna(' ',inplace=True)\nmerge['comment_text']=merge['comment_text'].apply(lambda x :clean(x))\nenrich_indirect_features(merge)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd6c6f0e-303a-4086-b04a-4fc69e3b1bdf","_uuid":"0e676c391bd50c3d9c4f2d38711aba64545bbd2c","collapsed":true,"trusted":true},"cell_type":"code","source":"# SELECTED_COLS = ['word_unique_percent','punct_percent']\n\n# # Word ngram vector\n# vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word', tokenizer=tokenize,\n#                         stop_words= 'english',ngram_range=(1,2),dtype=np.float32)\n# merge_word = vect_word.fit_transform(merge['comment_text'])\n# print('[{}] TFIDF vectorize `vect_word` completed.'.format(time.time() - start_time))\n\n# # Character n gram vector\n# vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n#                         stop_words= 'english',ngram_range=(2,6),dtype=np.float32)\n# merge_char = vect_char.fit_transform(merge['comment_text'])\n# print('[{}] TFIDF vectorize `vect_char` completed.'.format(time.time() - start_time))\n\n# num_clusters = 30 # need to be selected wisely\n# kmeans_model_char = MiniBatchKMeans(n_clusters=num_clusters,\n#                                init='k-means++',\n#                                n_init=1,\n#                                init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n# merge_char_kmeans = kmeans_model_char.fit_transform(merge_char)\n# print('[{}] K means clustering  `merge_char_kmeans` completed.'.format(time.time() - start_time))\n\n# sparse_merge = sparse.hstack([merge_word, merge_char_kmeans, merge[SELECTED_COLS]]).tocsr()\n\n# del merge_word, merge_char, merge, merge_char_kmeans\n# gc.collect()\n    \n\n# x_train = sparse_merge[:nrow_train]\n# x_test = sparse_merge[nrow_train:]\n\ntarget_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\ny_train = train[target_col].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32f51dcf-3038-4919-8882-609bca8793d4","_uuid":"2a131c52721ba75769c518780d76ec489663c622","collapsed":true,"trusted":true},"cell_type":"code","source":"# x_train[y_train['toxic'].values==1].sum(0)\n# (y_train['toxic'].values==0).sum()\n# def pr(x, y_i, y):\n#     p = x[y==y_i].sum(0)\n#     return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70a3ebc5-fe5f-4af2-a393-eb49dcce99fd","_uuid":"b94e22885e0ea65b7e1426dfcd83d854d3607f8f","collapsed":true,"trusted":true},"cell_type":"code","source":"# #Logistic Regression\n# prd = np.zeros((x_test.shape[0],y_train.shape[1]))\n# model_val = np.zeros((x_train.shape[0],y_train.shape[1]))\n# # cv_score =[]\n# for i,col in enumerate(target_col):\n#     r = sparse.csr_matrix(np.log(pr(x_train, 1,y_train[col]) / pr(x_train, 0,y_train[col])))\n#     x_train_nb = x_train.multiply(r)\n#     lr = LogisticRegression(C=2,random_state = i,class_weight = 'balanced')\n#     print('Building {} model for column:{''}'.format(i,col)) \n#     lr.fit(x_train_nb,y_train[col])\n#     #cv_score.append(lr.score)\n    \n#     prd[:,i] = lr.predict_proba(x_test.multiply(r))[:,1]\n# #     prd[:,i] = lr.predict_proba(x_test)[:,1]\n    \n#     model_val[:,i] = lr.predict(x_train_nb)\n# submission = pd.DataFrame(prd,columns=y_train.columns)\n# model_val = pd.DataFrame(model_val,columns=y_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70afe5f8-e53d-4f93-9177-8cf1ae9a96e7","_uuid":"2ec9b70455fb3c1f2dfc3f762368ac05e05a0285","collapsed":true,"trusted":true},"cell_type":"code","source":"# # LightGBM with Select K Best\n# for i,col in enumerate(target_col):\n#     print(col)\n#     train_target = y_train[col]\n#     train_sparse_matrix = x_train\n# #     model = LogisticRegression(solver='sag')\n# #     sfm = SelectFromModel(model, threshold=0.2)\n# #     train_sparse_matrix = sfm.fit_transform(x_train.tocoo(), train_target)\n# #     print(train_sparse_matrix.shape)\n#     train_sparse_matrix, valid_sparse_matrix, lgbm_y_train, lgbm_y_valid = train_test_split(train_sparse_matrix, train_target, test_size=0.05, random_state=144)\n# #     test_sparse_matrix = sfm.transform(x_test.tocoo())\n#     test_sparse_matrix = x_test\n#     d_train = lgb.Dataset(train_sparse_matrix, label=lgbm_y_train)\n#     d_valid = lgb.Dataset(valid_sparse_matrix, label=lgbm_y_valid)\n#     watchlist = [d_train, d_valid]\n#     params = {'learning_rate': 0.2,\n#               'application': 'binary',\n#               'num_leaves': 31,\n#               'verbosity': -1,\n#               'metric': 'auc',\n#               'data_random_seed': 2,\n#               'bagging_fraction': 0.8,\n#               'feature_fraction': 0.6,\n#               'nthread': 4,\n#               'lambda_l1': 1,\n#               'lambda_l2': 1}\n#     rounds_lookup = {'toxic': 140,\n#                  'severe_toxic': 50,\n#                  'obscene': 80,\n#                  'threat': 80,\n#                  'insult': 70,\n#                  'identity_hate': 80}\n#     model = lgb.train(params,\n#                       train_set=d_train,\n#                       num_boost_round=rounds_lookup[col],\n#                       valid_sets=watchlist,\n#                       verbose_eval=10)\n#     submission[col] = model.predict(test_sparse_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65c69d11-13f6-4798-b454-54a9d7399776","_uuid":"688be066d2fc1d4d9caa0377dfa09cefdad31865","collapsed":true,"trusted":true},"cell_type":"code","source":"# keras\nmax_features=30000\nmaxlen=150\nembed_size=300\n\nx_comment_train = merge[:nrow_train]['comment_text']\nx_comment_test = merge[nrow_train:]['comment_text']\nprint(\"x_train shape: \", x_comment_train.shape)\ntokenizer=text.Tokenizer(num_words=max_features,lower=True)\ntokenizer.fit_on_texts(list(x_comment_train)+list(x_comment_test))\nx_comment_train =tokenizer.texts_to_sequences(x_comment_train)\nx_comment_test =tokenizer.texts_to_sequences(x_comment_test)\n# print(\"x_train shape after to seq: \", x_train.shape)\nx_train_seq=sequence.pad_sequences(x_comment_train,maxlen=maxlen)\nx_test_seq=sequence.pad_sequences(x_comment_test,maxlen=maxlen)\nprint(\"x_train shape after pad sequence: \", x_train_seq.shape)\n\nx_train = pd.DataFrame(x_train_seq)\nx_train['word_unique_percent']  = merge['word_unique_percent'][:nrow_train]\nx_train['punct_percent'] = merge['punct_percent'][:nrow_train]\nx_test = pd.DataFrame(x_test_seq)\n# x_test['word_unique_percent']  = merge['word_unique_percent'][nrow_train:]\n# x_test['punct_percent'] = merge['punct_percent'][nrow_train:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a3bf28c-f1a7-40f6-bdae-16157e0d4922","_uuid":"812bac86e3d60b7fa3cbd5029e388a96b9121ff6","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n# embedding_matrix = np.zeros((nb_words, embed_size))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff73f5bf-9741-4d94-89f6-0644b70af7f5","_uuid":"d744c86a2a8b3f3932027931da60639507d74af4","collapsed":true,"trusted":true},"cell_type":"code","source":"sequence_input = Input(shape=(maxlen, ))\n# x = Embedding(max_features, embed_size)(sequence_input)\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(sequence_input)\n\ninput2 = Input(shape=(2,))\n\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(80, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n# x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\n\nx = concatenate([avg_pool, max_pool, input2]) # Add input2 into this layer\n# x = concatenate([avg_pool, max_pool])\n\n# x = Dense(128, activation='relu')(x)\n# x = Dropout(0.1)(x)\npreds = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=[sequence_input, input2], outputs=preds)\n# model = Model(inputs=sequence_input, outputs=preds)\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60c5277a-6d16-4731-be34-824d97a061d8","_uuid":"217d433ce320147ab1f3102b9b60cebdf54e0542","collapsed":true,"trusted":true},"cell_type":"code","source":"batch_size = 64\nepochs = 4\nprint(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\nx_input2 = X_tra[['word_unique_percent', 'punct_percent']]\nX_val2 = X_val[['word_unique_percent', 'punct_percent']]\nX_tra = X_tra.drop(columns=['word_unique_percent', 'punct_percent'])\nX_val = X_val.drop(columns=['word_unique_percent', 'punct_percent'])\n\nx_test2 = merge[['word_unique_percent', 'punct_percent']][nrow_train:]\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"faca48d6-57e0-4034-ab5b-dbed62a46054","_uuid":"c2b49b64e08825b949e5f93af86e0d71ce6e2aac","collapsed":true,"trusted":true},"cell_type":"code","source":"filepath=\"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\nra_val = RocAucEvaluation(validation_data=([X_val,X_val2], y_val), interval = 1)\n# ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\ncallbacks_list = [ra_val,checkpoint, early]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a9dec50-4ef2-43cf-b6a3-7cac782eeb35","_uuid":"e4343e26801cc534c4e362bd77cce1f5e2679caa","collapsed":true,"trusted":true},"cell_type":"code","source":"model.fit([X_tra,x_input2], y_tra, batch_size=batch_size, epochs=epochs, validation_data=([X_val,X_val2], y_val),callbacks = callbacks_list,verbose=1)\n# model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n#Loading model weights\nmodel.load_weights(filepath)\nprint('Predicting....')\ny_pred = model.predict([x_test,x_test2],batch_size=1024,verbose=1)\n# y_pred = model.predict(x_test,batch_size=1024,verbose=1)\nsubmission = submission.reindex(columns=[\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9b9b78b-30da-4a73-91f6-f2ac7c286a31","_uuid":"cfa871e2707cf4a7ad2e89af1cacc4e8882f9b1c","collapsed":true,"trusted":true},"cell_type":"code","source":"#Model validation\n# score = roc_auc_score(model_val, y_train)\n# print(\"\\n ROC-AUC - score: %.6f \\n\" % (score))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c323b8ea-014f-4f76-aea2-86b4448641c5","_uuid":"45472f0033050853bb3af1fdee14505d1d57dd8b","collapsed":true,"trusted":true},"cell_type":"code","source":"submission.to_csv('toxic_comment_classification.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}