{"cells":[{"metadata":{"_cell_guid":"a698e161-b715-40c6-bc19-4c2ebafc3bea","_uuid":"939b449cc8f6210dc25ffa56caf00c5d5f184f1e","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport time\nimport os.path\nimport gc\nimport matplotlib.pyplot as plt\nimport string\nfrom scipy.sparse import csr_matrix, hstack\nimport sys\n%matplotlib inline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\n\n#Add https://www.kaggle.com/anttip/wordbatch to your kernel Data Sources, \n#until Kaggle admins fix the wordbatch pip package installation\nsys.path.insert(0, '../input/wordbatch/wordbatch/')\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom nltk.corpus import stopwords\nimport re\n\nNAME_MIN_DF = 10\nMAX_FEATURES_ITEM_DESCRIPTION = 1200000\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\n    \ndef handle_missing_inplace(dataset):\n    dataset['general_cat'].fillna(value='No Label', inplace=True)\n    dataset['subcat_1'].fillna(value='No Label', inplace=True)\n    dataset['subcat_2'].fillna(value='No Label', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='No description yet', inplace=True)\n\n\ndef cutting(dataset):\n    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing']\n    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'No Label']\n    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'No Label']\n    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'No Label']\n    pop_brand= pop_brand[pop_brand!=1].index\n    pop_category1 = pop_category1[pop_category1!=1].index\n    pop_category2 = pop_category2[pop_category2!=1].index\n    pop_category3 = pop_category3[pop_category3!=1].index\n    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'No Label'\n    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'No Label'\n    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'No Label'\n    \ndef to_categorical(dataset):\n    dataset['general_cat'] = dataset['general_cat'].astype('category')\n    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n    \ndef rmsle(y, y_pred):\n    y = np.array(y).reshape(-1)\n    y_pred = np.array(y_pred).reshape(-1)\n    assert len(y) == len(y_pred)\n    to_sum = (np.log(y_pred + 1) - np.log(y + 1)) ** 2\n    return to_sum.mean() ** 0.5\n#Source: https://www.kaggle.com/marknagelberg/rmsle-function\n\n# Define helpers for text normalization\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64d7ad8a-578f-47ec-96ed-b8dc6cff6191","_uuid":"11094a9eb86d338b16266c9e879eeb4df9a15a5d","collapsed":true,"trusted":true},"cell_type":"code","source":"# sorted(merge['subcat_2'].unique())'description' in \n# merge[merge.item_description.str.contains('No')]['item_description'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a275dfa-e2a9-401e-b004-c93f7e6d639e","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"5493583208309522b8f80510a2a35400798c5651","collapsed":true,"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(20, 15))\n# bins=50\n# # plt.hist(train2['price'], bins, normed=True, range=[0,250],\n# #          alpha=0.6, label='price when shipping==1')\n# plt.hist(train2[(train2.price < 1.0)]['price'], bins, normed=True, range=[0,1],\n#          alpha=0.6, label='price when shipping==0')\n# plt.title('Train price over shipping type distribution', fontsize=15)\n# plt.xlabel('Price', fontsize=15)\n# plt.ylabel('Normalized Samples', fontsize=15)\n# plt.legend(fontsize=15)\n# plt.xticks(fontsize=15)\n# plt.yticks(fontsize=15)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0b55041-e7cd-410d-b9a6-c13788c2e403","_uuid":"44c8d5eb5e1d41d89323cc5470c4199caf57652c","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"# print(merge[merge.subcat_1 == 'Artwork']['general_cat'].unique())\n# print(merge[merge.subcat_1 == 'Art']['general_cat'].unique())\n# print(merge[merge.subcat_1 == 'Artwork']['subcat_2'].unique())\n# print(merge[merge.subcat_1 == 'Art']['subcat_2'].unique())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"423a465d-5d56-48f1-b7c4-376f9ba7aee8","_uuid":"9b3005093665e33d9bb70642f7e8cde4a5f3a9df","collapsed":true,"trusted":true},"cell_type":"code","source":"if os.path.exists(\"data.pkl\"):\n    print(\"Loading saved dataset, data.pkl.\")\n    X,X_test,y = pickle.load(open(\"data.pkl\",\"rb\"))\nelse:\n    start_time = time.time()\n\n    train = pd.read_table('../input/train.tsv', engine='c')\n    test = pd.read_table('../input/test.tsv', engine='c')\n    print('[{}] Finished to load data'.format(time.time() - start_time))\n    print('Train shape: ', train.shape)\n    print('Test shape: ', test.shape)\n    nrow_test = train.shape[0] #-dftt.shape[0]\n    dftt = train[(train.price < 0.1)]\n    train = train.drop(train[(train.price < 0.1)].index)\n    del dftt['price']\n    nrow_train = train.shape[0] #-dftt.shape[0]\n    #nrow_test = train.shape[0] + dftt.shape[0]\n\n    y_original = train[\"price\"] # save down original value\n    # Scale output price -> log -> minMax(-1,1)\n    price_scaler = MinMaxScaler(feature_range=(-1, 1))\n    train_price_log = np.log1p(train[\"price\"]) # sames as np.log(train['price'] + 1)\n\n    y = train_price_log\n    # y = price_scaler.fit_transform(train_price_log.values.reshape(-1, 1))\n\n    merge: pd.DataFrame = pd.concat([train, dftt, test])\n    submission: pd.DataFrame = test[['test_id']]\n\n    del train\n    del test\n    gc.collect()\n\n    merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n    zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n    merge.drop('category_name', axis=1, inplace=True)\n    print('[{}] Split categories completed.'.format(time.time() - start_time))\n\n    handle_missing_inplace(merge)\n    print('[{}] Handle missing completed.'.format(time.time() - start_time))\n\n    cutting(merge)\n    print('[{}] Cut completed.'.format(time.time() - start_time))\n\n    to_categorical(merge)\n    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n\n    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n                                                                  \"idf\": None,\n                                                                  }), procs=8)\n    wb.dictionary_freeze= True\n    X_name = wb.fit_transform(merge['name'])\n    del(wb)\n    print('[{}] Count vectorize `name` completed.'.format(time.time() - start_time))\n\n    cv = CountVectorizer()\n    X_category1 = cv.fit_transform(merge['general_cat'])\n    X_category2 = cv.fit_transform(merge['subcat_1'])\n    X_category3 = cv.fit_transform(merge['subcat_2'])\n    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n\n    tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n                         ngram_range=(1, 2),\n                         stop_words='english')\n    X_description = tv.fit_transform(merge['item_description'])\n    print('[{}] TFIDF vectorize `item_description` completed.'.format(time.time() - start_time))\n\n    num_clusters = 30 # need to be selected wisely\n    kmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                                   init='k-means++',\n                                   n_init=1,\n                                   init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n    X_description_kmeans = kmeans_model.fit_transform(X_description)\n    print('[{}] K means clustering  `X_description_kmeans` completed.'.format(time.time() - start_time))\n\n    lb = LabelBinarizer(sparse_output=True)\n    X_brand = lb.fit_transform(merge['brand_name'])\n    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n\n    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n\n    print (X_dummies.shape, X_description_kmeans.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape, X_name.shape)\n    sparse_merge = hstack((X_dummies, X_description_kmeans, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n    del X_dummies, merge, X_description, X_brand, X_category1, X_category2, X_category3, X_name, X_description_kmeans\n    gc.collect()\n\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n\n    # valid_y_original = price_scaler.inverse_transform(valid_y)\n    # valid_y_original = np.expm1(valid_y_original)\n    # valid_y_original = valid_y_original.clip(0, None)\n\n    pickle.dump((X,X_test,y), open(\"MNIST.pkl\",\"wb\"))\n    \n# train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.1, random_state = 144)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"621f922f-49ed-4291-9484-b2584ab60565","_uuid":"62dd10702a7dada5306e04e3147780f8e3b0ddf3","collapsed":true,"trusted":true},"cell_type":"code","source":"#  create a dictionary mapping the tokens to their tfidf values\n\n# tfidf = dict(zip(tv.get_feature_names(), tv.idf_))\n# tfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n#                     dict(tfidf), orient='index')\n# tfidf.columns = ['tfidf']\n# tfidf.sort_values(by=['tfidf'], ascending=True).tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f84fcd6-9302-467e-9d89-5c43cdf2ef0b","_uuid":"6762eceddf60d7fd022b5373b36a8c4ca466d352","collapsed":true,"trusted":true},"cell_type":"code","source":"# # Ridge 1\n# model = Ridge(alpha=.6, copy_X=True, fit_intercept=True, max_iter=100,\n# normalize=False, random_state=101, solver='auto', tol=0.01)\n# model.fit(X, y)\n# print('[{}] Train ridge completed'.format(time.time() - start_time))\n# predsR_train = model.predict(X=X)\n# # predsR_valid = model.predict(X=valid_X)\n# predsR = model.predict(X=X_test)\n# print('[{}] Predict ridge completed'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d0ccb48-2454-4a27-a18a-b2704459dc9e","_uuid":"a07e36204441692e38740048df3ff0ecf54b989c","collapsed":true,"trusted":true},"cell_type":"code","source":"# # Ridge 2\n# model = Ridge(solver='sag', fit_intercept=True)\n# model.fit(X, y)\n# print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n# predsR2_train = model.predict(X=X)\n# # predsR2_valid = model.predict(X=valid_X)\n# predsR2 = model.predict(X=X_test)\n# print('[{}] Predict ridge v2 completed'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0da0847-06ee-45a8-a1be-aadfdb220341","_uuid":"e2f59dd3b85b38cfa283fa99561fff97d8b3f41d","collapsed":true,"trusted":true},"cell_type":"code","source":"# # LGBM 1\n# # train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.1, random_state = 144) \n# d_train = lgb.Dataset(train_X, label=train_y.reshape(-1))\n# d_valid = lgb.Dataset(valid_X, label=valid_y.reshape(-1))\n# watchlist = [d_train, d_valid]\n\n# params = {\n#     'learning_rate': 0.7,\n#     'application': 'regression',\n#     'max_depth': 3,\n#     'num_leaves': 60,\n#     'verbosity': -1,\n#     'metric': 'RMSE',\n#     'data_random_seed': 1,\n#     'reg_alpha': 1,\n#     'min_split_gain': 0.5, \n#     'min_child_weight': 1, \n#     'min_child_samples': 10, \n#     'scale_pos_weight': 1,\n#     'reg_lambda': 0.001,\n#     'bagging_fraction': 0.8,\n#     'nthread': 4\n# }\n# model = lgb.train(params, train_set=d_train, num_boost_round=7500, valid_sets=watchlist, \\\n# early_stopping_rounds=1000, verbose_eval=1000)\n# predsL_train = model.predict(X)\n# # predsL_valid = model.predict(valid_X)\n# predsL = model.predict(X_test)\n\n# print('[{}] Predict lgb 1 completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"123cece7-bea7-429f-b854-12d9890bed06","_uuid":"9a9ae3a1df42281f2208d290afa12e8d625b5cc6","collapsed":true,"trusted":true},"cell_type":"code","source":"# LGBM 2\ntrain_X2, valid_X2, train_y2, valid_y2 = train_test_split(X, y, test_size = 0.1, random_state = 101) \nd_train2 = lgb.Dataset(train_X2, label=train_y2.reshape(-1))\nd_train2 = lgb.Dataset(valid_X2, label=valid_y2.reshape(-1))\nwatchlist2 = [d_train2, d_train2]\n\nparams2 = {\n    'learning_rate': 0.85,\n    'application': 'regression',\n    'max_depth': 3,\n    'num_leaves': 130,\n    'verbosity': -1,\n    'metric': 'RMSE',\n    'data_random_seed': 2,\n    'reg_alpha': 1, \n    'reg_lambda': 0.001,\n    'save_binary': True,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 2,\n    'min_split_gain': 0.05, \n#     'min_child_weight': 1, \n    'min_child_samples': 10, \n    'nthread': 4\n}\n\nmodel = lgb.train(params2, train_set=d_train2, num_boost_round=5000, valid_sets=watchlist2, \\\nearly_stopping_rounds=500, verbose_eval=500)\npredsL2_train = model.predict(X)\n# predsL2_valid = model.predict(valid_X)\npredsL2 = model.predict(X_test)\n\nprint('[{}] Predict lgb 2 completed.'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6383468d-c820-4c00-8787-4bb5e6a48c6e","_uuid":"1a47873a3e787dd4f4068ab1b1882710d98094ef","collapsed":true,"trusted":true},"cell_type":"code","source":"regr = RandomForestRegressor(max_depth=4, random_state=0, n_estimators = 20, min_samples_leaf = 50)\n\n# regr_X_train = pd.DataFrame(data={'predsR_train':predsR_train.reshape(-1), 'predsR2_train':predsR2_train.reshape(-1),\\\n#                                   'predsL2_train':predsL2_train.reshape(-1)})\nregr_X_train = pd.DataFrame(data={'predsL2_train':predsL2_train.reshape(-1)})\n\nregr.fit(regr_X_train, y.reshape(-1))\n# regr_X_valid = pd.DataFrame(data={'predsR_valid':predsR_valid.reshape(-1), 'predsR2_valid':predsR2_valid.reshape(-1),\\\n#                                   'predsL_valid':predsL_valid.reshape(-1), 'predsL2_valid':predsL2_valid.reshape(-1)})\n\n# regr_X_test = pd.DataFrame(data={'predsR':predsR.reshape(-1), 'predsR2':predsR2.reshape(-1),\\\n#                                  'predsL2':predsL2.reshape(-1)})\nregr_X_test = pd.DataFrame(data={'predsL2':predsL2.reshape(-1)})\n\n# predRegr_valid = regr.predict(regr_X_valid)\npredRegr_test = regr.predict(regr_X_test)\nprint('[{}] Random Forest Regressor completed'.format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28aca61a-4f1b-4167-8c2d-8dc5acbdaec9","_uuid":"f4b123da88dfc1ccaed727c74c068f9ae32c2822","collapsed":true,"trusted":true},"cell_type":"code","source":"# preds = (predsR.reshape(-1)*0.2 + predsL.reshape(-1)*0.3 + predsL2.reshape(-1)*0.3 + predsR2.reshape(-1)*0.2)\n# preds_valid = (predsR_valid.reshape(-1)*0.2 + predsL_valid.reshape(-1)*0.3 + predsL2_valid.reshape(-1)*0.3 + predsR2_valid.reshape(-1)*0.2)\n\n# Scale back price\n# pred_scale_back = price_scaler.inverse_transform(preds.reshape(-1, 1))\n# pred_scale_back = np.expm1(pred_scale_back)\n# pred_scale_back = pred_scale_back.clip(0, None)\n\n# preds_valid_scaled = price_scaler.inverse_transform(preds_valid.reshape(-1, 1))\n# preds_valid_scaled = np.expm1(preds_valid_scaled)\n# preds_valid_scaled = preds_valid_scaled.clip(0, None)\n\n# predRegr_valid_scaled = price_scaler.inverse_transform(predRegr_valid.reshape(-1, 1))\n# predRegr_valid_scaled = np.expm1(predRegr_valid_scaled)\n# predRegr_valid_scaled = predRegr_valid_scaled.clip(0, None)\n\n# predRegr_test_scaled = price_scaler.inverse_transform(predRegr_test.reshape(-1, 1))\npredRegr_test_scaled = np.expm1(predRegr_test)\npredRegr_test_scaled = predRegr_test_scaled.clip(0, None)\n\nsubmission['price'] = predRegr_test_scaled\nsubmission.to_csv(\"submission_ridge_2xlgbm_rfRegr.csv\", index=False)\nprint('writting is done.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}